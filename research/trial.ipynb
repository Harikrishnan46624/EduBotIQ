{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.document_loaders.csv_loader import CSVLoader \n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data, \n",
    "                             glob='*.csv', \n",
    "                             loader_cls=CSVLoader)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(r\"E:\\projects\\EduBotIQ\\Data Prepration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n",
    "#Create text chunk\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of my chunk: 2847\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello World\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(text_chunks, embeddings):\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    \n",
    "    db = FAISS.from_documents(text_chunks, embeddings)\n",
    "    db.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_db(text_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=': 4\\nquestion: What is object recognition in Computer Vision?', metadata={'source': 'E:\\\\projects\\\\EduBotIQ\\\\Data Prepration\\\\new_train.csv', 'row': 4}), Document(page_content=': 18\\nquestion: How are object detection models evaluated in Computer Vision?', metadata={'source': 'E:\\\\projects\\\\EduBotIQ\\\\Data Prepration\\\\new_train.csv', 'row': 18}), Document(page_content='answer: Object recognition is a Computer Vision task that involves identifying and classifying objects in images or videos. Convolutional Neural Networks (CNNs) are often used for object recognition, enabling machines to recognize and categorize objects within a visual scene.', metadata={'source': 'E:\\\\projects\\\\EduBotIQ\\\\Data Prepration\\\\new_train.csv', 'row': 4})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "# Load the FAISS database with the embeddings\n",
    "db = FAISS.load_local(\"vectorstore/db_faiss\", embeddings=embeddings)\n",
    "\n",
    "# Encode your query text\n",
    "query_text = \"What is Object Detection\"\n",
    "\n",
    "docs=db.similarity_search(query_text, k=3)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=r\"E:\\projects\\EduBotIQ\\tiny_model\\tinyllama-1.1b-chat-v1.0.Q8_0.gguf\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':256,\n",
    "                          'temperature':0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "# Load the FAISS database with the embeddings\n",
    "db = FAISS.load_local(\"vectorstore/db_faiss\", embeddings=embeddings)\n",
    "\n",
    "\n",
    "retriever=db.as_retriever(search_kwargs={'k': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=db.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\projects\\EduBotIQ\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response :  Object Detection Models Evaluated in Computer Vision\n",
      "\n",
      "Object detection is a fundamental task in computer vision, where an algorithm is trained to identify objects of interest in images or videos. Object detection models are typically trained on large datasets containing labeled images with their corresponding object labels. The goal of object detection is to accurately predict the location and size of each object in the image or video.\n",
      "\n",
      "Different types of object detection models can be categorized based on their architecture, training approach, and performance metrics. Some common types include:\n",
      "\n",
      "1. Fully Convolutional Networks (FCN)\n",
      "2. ResNet\n",
      "3. Inception-v4\n",
      "4. MobileNetV2\n",
      "5. ShuffleNet\n",
      "6. YOLOv3\n",
      "7. SSD\n",
      "8. RetinaNet\n",
      "9. Faster R-CNN\n",
      "10. PASCAL VOC\n",
      "\n",
      "In this context, the question asks about object detection models evaluated in computer vision. Object detection models are evaluated using various metrics such as precision, recall, accuracy, and F1 score. These metrics help to determine the model's performance on a particular task or dataset. The evaluation process involves training the model on a large\n",
      "Response :  Random Forest Regression is an ensemble learning algorithm that combines multiple decision trees during training to improve accuracy, reduce overfitting and handle high dimensional data. It uses a random subset of features for each split in the decision tree, which introduces randomness into the model. The algorithm builds a set of decision trees by selecting features randomly, with the aim of creating models that can learn complex relationships between variables. The final output is a weighted average of these predictions. This approach helps to reduce overfitting and improve generalization.\n",
      "Response :  Random Forest is an ensemble learning algorithm that constructs multiple decision trees during training to improve accuracy and reduce overfiting. It combines predictions from these trees through a voting mechanism to improve generalization. Random Forest is widely used in handling high-dimensional data and is effective in handling nonlinear relationships.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "    result=qa({\"query\": user_input})\n",
    "    print(\"Response : \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
